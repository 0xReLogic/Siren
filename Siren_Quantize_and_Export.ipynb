{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Siren: Baseline Model Training, Quantization, and ONNX Export\n",
       "\n",
       "This notebook covers the complete pipeline:\n",
       "1.  **Setup**: Install dependencies and upload data.\n",
       "2.  **Training**: Fine-tune a DistilBERT model on the dataset.\n",
       "3.  **Quantization**: Apply dynamic quantization to reduce model size and speed up inference.\n",
       "4.  **ONNX Export**: Convert the quantized model to the ONNX format for cross-platform use.\n",
       "5.  **Verification**: Load the ONNX model and test it to ensure the export was successful."
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 1. Setup"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Install necessary libraries\n",
       "!pip install transformers pandas torch\n",
       "!pip install onnx onnxruntime"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Upload the dummy_data.csv file\n",
       "from google.colab import files\n",
       "\n",
       "uploaded = files.upload()\n",
       "\n",
       "for fn in uploaded.keys():\n",
       "  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 2. Model Training"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import pandas as pd\n",
       "import torch\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
       "from torch.optim import AdamW\n",
       "from sklearn.model_selection import train_test_split\n",
       "\n",
       "def train_baseline_model(file_path):\n",
       "    # Load Data\n",
       "    df = pd.read_csv(file_path)\n",
       "    train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
       "        df['url'], df['is_phishing'], test_size=0.2, random_state=42\n",
       "    )\n",
       "\n",
       "    # Tokenizer\n",
       "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
       "\n",
       "    class PhishingDataset(Dataset):\n",
       "        def __init__(self, encodings, labels):\n",
       "            self.encodings = encodings\n",
       "            self.labels = labels\n",
       "\n",
       "        def __getitem__(self, idx):\n",
       "            item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
       "            item['labels'] = torch.tensor(self.labels[idx])\n",
       "            return item\n",
       "\n",
       "        def __len__(self):\n",
       "            return len(self.labels)\n",
       "\n",
       "    train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=512)\n",
       "    val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=512)\n",
       "\n",
       "    train_dataset = PhishingDataset(train_encodings, list(train_labels))\n",
       "    val_dataset = PhishingDataset(val_encodings, list(val_labels))\n",
       "\n",
       "    # Model\n",
       "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
       "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
       "    model.to(device)\n",
       "\n",
       "    # Training\n",
       "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
       "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
       "\n",
       "    model.train()\n",
       "    for epoch in range(3):  # 3 epochs for fine-tuning\n",
       "        for batch in train_loader:\n",
       "            optimizer.zero_grad()\n",
       "            input_ids = batch['input_ids'].to(device)\n",
       "            attention_mask = batch['attention_mask'].to(device)\n",
       "            labels = batch['labels'].to(device)\n",
       "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
       "            loss = outputs.loss\n",
       "            loss.backward()\n",
       "            optimizer.step()\n",
       "        print(f'Epoch {epoch+1} | Loss: {loss.item()}')\n",
       "\n",
       "    print('Finished Training')\n",
       "    return model, tokenizer\n",
       "\n",
       "# Run training\n",
       "trained_model, tokenizer = train_baseline_model('dummy_data.csv')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 3. Quantization"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Move model to CPU for quantization\n",
       "trained_model.to('cpu')\n",
       "\n",
       "# Apply dynamic quantization\n",
       "quantized_model = torch.quantization.quantize_dynamic(\n",
       "    trained_model, {torch.nn.Linear}, dtype=torch.qint8\n",
       ")\n",
       "\n",
       "print('Model successfully quantized.')\n",
       "# You can print the model to see the difference\n",
       "# print(trained_model)\n",
       "# print(quantized_model)"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 4. ONNX Export"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import torch\n",
       "\n",
       "# Prepare a dummy input for the exporter\n",
       "dummy_input = tokenizer('this is a sample url', return_tensors='pt')\n",
       "input_ids = dummy_input['input_ids']\n",
       "attention_mask = dummy_input['attention_mask']\n",
       "\n",
       "onnx_model_path = 'siren_model.onnx'\n",
       "\n",
       "# Export the model\n",
       "torch.onnx.export(\n",
       "    quantized_model, \n",
       "    (input_ids, attention_mask), \n",
       "    onnx_model_path, \n",
       "    export_params=True, \n",
       "    opset_version=11, \n",
       "    do_constant_folding=True, \n",
       "    input_names=['input_ids', 'attention_mask'],\n",
       "    output_names=['output'],\n",
       "    dynamic_axes={'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
       "                  'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
       "                  'output': {0: 'batch_size'}}\n",
       ")\n",
       "\n",
       "print(f'Model exported to {onnx_model_path}')"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 5. Verification"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "import onnxruntime\n",
       "import numpy as np\n",
       "\n",
       "# Create an ONNX runtime session\n",
       "ort_session = onnxruntime.InferenceSession(onnx_model_path)\n",
       "\n",
       "# Prepare the dummy input in the format ONNX runtime expects (numpy arrays)\n",
       "ort_inputs = {\n",
       "    'input_ids': input_ids.numpy(),\n",
       "    'attention_mask': attention_mask.numpy()\n",
       "}\n",
       "\n",
       "# Run inference\n",
       "ort_outs = ort_session.run(None, ort_inputs)\n",
       "\n",
       "print('ONNX model loaded and verified successfully!')\n",
       "print('Output shape:', ort_outs[0].shape)\n",
       "print('Output logits:', ort_outs[0])"
      ]
     },
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "## 6. Download the ONNX Model"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "from google.colab import files\n",
       "\n",
       "files.download(onnx_model_path)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }