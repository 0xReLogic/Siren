{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Siren - Baseline Model Training (DistilBERT)\n",
       "\n",
       "This notebook is for training the baseline phishing detection model using DistilBERT on Google Colab.\n",
       "\n",
       "**Steps:**\n",
       "1.  Run the first code cell to install the necessary libraries.\n",
       "2.  Run the second code cell and upload the [dummy_data.csv](cci:7://file:///d:/Project%20Utama/Rust/Siren/model/data/dummy_data.csv:0:0-0:0) file when prompted.\n",
       "3.  Run the final cell to start the training process."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 1. Install Dependencies\n",
       "!pip install -q transformers pandas torch"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 2. Upload Data\n",
       "from google.colab import files\n",
       "\n",
       "print('Please upload the dummy_data.csv file.')\n",
       "uploaded = files.upload()\n",
       "\n",
       "# Check if the file was uploaded\n",
       "for fn in uploaded.keys():\n",
       "  print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
       "  DATA_PATH = fn"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# 3. Training Script\n",
       "import pandas as pd\n",
       "import torch\n",
       "from torch.utils.data import Dataset, DataLoader\n",
       "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
       "\n",
       "# Configuration\n",
       "MODEL_NAME = 'distilbert-base-uncased'\n",
       "NUM_EPOCHS = 3\n",
       "BATCH_SIZE = 2\n",
       "LEARNING_RATE = 5e-5\n",
       "\n",
       "# Custom Dataset Class\n",
       "class PhishingDataset(Dataset):\n",
       "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
       "        self.texts = texts\n",
       "        self.labels = labels\n",
       "        self.tokenizer = tokenizer\n",
       "        self.max_len = max_len\n",
       "\n",
       "    def __len__(self):\n",
       "        return len(self.texts)\n",
       "\n",
       "    def __getitem__(self, item):\n",
       "        text = str(self.texts[item])\n",
       "        label = self.labels[item]\n",
       "\n",
       "        encoding = self.tokenizer.encode_plus(\n",
       "            text,\n",
       "            add_special_tokens=True,\n",
       "            max_length=self.max_len,\n",
       "            return_token_type_ids=False,\n",
       "            padding='max_length',\n",
       "            return_attention_mask=True,\n",
       "            return_tensors='pt',\n",
       "            truncation=True\n",
       "        )\n",
       "\n",
       "        return {\n",
       "            'text': text,\n",
       "            'input_ids': encoding['input_ids'].flatten(),\n",
       "            'attention_mask': encoding['attention_mask'].flatten(),\n",
       "            'labels': torch.tensor(label, dtype=torch.long)\n",
       "        }\n",
       "\n",
       "# Main Training Function\n",
       "def train_model():\n",
       "    print(\"--- Starting Baseline Model Training ---\")\n",
       "\n",
       "    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
       "    model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
       "\n",
       "    df = pd.read_csv(DATA_PATH)\n",
       "    print(f\"Loaded {len(df)} records from {DATA_PATH}\")\n",
       "\n",
       "    dataset = PhishingDataset(\n",
       "        texts=df.text.to_numpy(),\n",
       "        labels=df.label.to_numpy(),\n",
       "        tokenizer=tokenizer\n",
       "    )\n",
       "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
       "\n",
       "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
       "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
       "    model.to(device)\n",
       "\n",
       "    print(f\"Using device: {device}\")\n",
       "    print(\"--- Setup complete. Starting training loop ---\")\n",
       "\n",
       "    model.train()\n",
       "    for epoch in range(NUM_EPOCHS):\n",
       "        print(f'\\nEpoch {epoch + 1}/{NUM_EPOCHS}')\n",
       "        for batch in dataloader:\n",
       "            input_ids = batch[\"input_ids\"].to(device)\n",
       "            attention_mask = batch[\"attention_mask\"].to(device)\n",
       "            labels = batch[\"labels\"].to(device)\n",
       "\n",
       "            outputs = model(\n",
       "                input_ids=input_ids,\n",
       "                attention_mask=attention_mask,\n",
       "                labels=labels\n",
       "            )\n",
       "            loss = outputs.loss\n",
       "\n",
       "            loss.backward()\n",
       "            optimizer.step()\n",
       "            optimizer.zero_grad()\n",
       "\n",
       "            print(f\"  - Batch processed. Loss: {loss.item():.4f}\")\n",
       "\n",
       "    print(\"\\n--- Baseline model training finished ---\")\n",
       "\n",
       "# Run training\n",
       "train_model()"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": "3.10.12"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }